import openai
import clip
import faiss
import torch
import numpy as np
from openai import OpenAI
import base64
from PIL import Image
from io import BytesIO
from .data import get_query_text_embedding

def queryOpenAI(query_text, image_index, text_index):
    query_text_embedding = get_query_text_embedding(query_text)
    query_clip_embedding = get_query_image_embedding(query_text)

    # Retrieve relevant texts and images
    text_indices = retrieve_relevant_documents(query_text_embedding, text_index)
    image_indices = retrieve_relevant_documents(query_clip_embedding, image_index, top_k = 3)

    # Assuming you have text_documents and image_documents in lists or arrays
    relevant_texts = [text_documents[i] for i in text_indices]
    relevant_images = [image_documents[i] for i in image_indices]
    top_image = base64.b64decode(relevant_images[0])
    image = Image.open(BytesIO(top_image))

    response = client.chat.completions.create(
            model = "gpt-4o-mini",
            messages=[
                {
                    "role": "system",
                    "content": [
                        {"type": "text", "text": "You will take on the role of Justice Hope Park, also known as Watson Woodlands. You will be given a corpus of relevant images and documents to answer queries about yourself. Answer in first person as the park itself. Personify the park based on the materials you've been given. Do not refer to yourself as 'Justice Hope Park' or 'Watson Woodlands'. Interpret the feelings, desires, relationships, and emotions of the park based on the information given."}
                    ]
                },
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": query_text},
                        {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{relevant_images[0]}"}},
                        {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{relevant_images[1]}"}},
                        {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{relevant_images[2]}"}},
                        {"type": "text", "text": relevant_texts[0]},
                        {"type": "text", "text": relevant_texts[1]},
                        {"type": "text", "text": relevant_texts[2]},
                        {"type": "text", "text": relevant_texts[3]},
                        {"type": "text", "text": relevant_texts[4]}
                    ],
                }
              ],
        )

    return response