{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "309e81f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import torch\n",
    "import pymupdf\n",
    "import pdfplumber\n",
    "from PIL import Image\n",
    "import io\n",
    "import pytesseract\n",
    "import pandas as pd\n",
    "import camelot\n",
    "import faiss\n",
    "import clip\n",
    "import numpy as np\n",
    "from transformers import GPT2Tokenizer\n",
    "from sklearn.decomposition import PCA\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acc44be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_archive = []\n",
    "image_archive = []\n",
    "text_metadata = []\n",
    "image_metadata = []\n",
    "\n",
    "def get_text_embedding(text):\n",
    "    response = openai.embeddings.create(input=text, model=\"text-embedding-3-small\")\n",
    "    return response\n",
    "\n",
    "# def get_text_embedding(text):\n",
    "#     device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#     model, _ = clip.load(\"ViT-B/32\", device=device)\n",
    "#     text_preprocessed = clip.tokenize([text]).to(device)\n",
    "#     with torch.no_grad():\n",
    "#         text_features = model.encode_text(text_preprocessed)\n",
    "#     return text_features.cpu().numpy()\n",
    "\n",
    "def get_image_embedding(image):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "    img_preprocessed = preprocess(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(img_preprocessed)\n",
    "    return image_features.cpu().numpy()\n",
    "\n",
    "    return image_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78e36ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, max_tokens=1000):\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "    sentences = text.split('.')\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    chunks = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        tokens = len(tokenizer.encode(sentence))\n",
    "        if current_length + tokens > max_tokens:\n",
    "            chunks.append('.'.join(current_chunk) + '.')\n",
    "            current_chunk = []\n",
    "            current_length = 0\n",
    "        \n",
    "        current_chunk.append(sentence)\n",
    "        current_length += tokens\n",
    "\n",
    "    # Add the last chunk\n",
    "    if current_chunk:\n",
    "        chunks.append('.'.join(current_chunk) + '.')\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def clean_text(text):\n",
    "    cleaned_text = text.replace('\\n', ' ').replace('\\r', '')\n",
    "    cleaned_text = ' '.join(cleaned_text.split())  # Remove excessive spaces\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acb1d489",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ocr_images(images):\n",
    "    ocr_texts = []\n",
    "    for image in images:\n",
    "        ocr_text = pytesseract.image_to_string(image)\n",
    "        cleaned_ocr_text = clean_text(ocr_text)\n",
    "        chunked_ocr_texts = chunk_text(cleaned_ocr_text)\n",
    "        ocr_texts = chunked_ocr_texts\n",
    "    \n",
    "    return ocr_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26a0618a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract text, images, and tables from a PDF and clean the extracted text\n",
    "def extract_pdf_content(pdf_path):\n",
    "    doc = pymupdf.open(pdf_path)\n",
    "    text_chunks = []\n",
    "    textMetadata = []\n",
    "    images = []\n",
    "    imageMetadata = []\n",
    "    tables = []\n",
    "    \n",
    "    file_name = pdf_path.replace('.pdf', '').replace('Data/', '')\n",
    "\n",
    "    # Process each page using PyMuPDF and pdfplumber\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page_num in range(len(doc)):\n",
    "            page = doc.load_page(page_num)\n",
    "            plumber_page = pdf.pages[page_num]\n",
    "\n",
    "            # Extract text\n",
    "            text = page.get_text()\n",
    "            if text:\n",
    "                cleaned_text = clean_text(text)\n",
    "                chunks = chunk_text(cleaned_text)\n",
    "                text_chunks.extend(chunks)\n",
    "                for chunk in chunks:\n",
    "                    metadata = {\n",
    "                        \"file\": file_name,\n",
    "                        \"page_number\": page_num + 1,  # 1-based index\n",
    "                        \"chunk_length\": len(chunk)\n",
    "                    }\n",
    "                    textMetadata.append(metadata)\n",
    "\n",
    "            # Extract images\n",
    "            for img_index, img in enumerate(page.get_images(full=True)):\n",
    "                xref = img[0]\n",
    "                base_image = doc.extract_image(xref)\n",
    "                image_bytes = base_image[\"image\"]\n",
    "                image_ext = base_image[\"ext\"]\n",
    "                image = Image.open(io.BytesIO(image_bytes))\n",
    "                images.append(image)\n",
    "                metadata = {\n",
    "                    \"file\": file_name,\n",
    "                    \"page_number\": page_num + 1,  # 1-based index\n",
    "                    \"image_index\": img_index,\n",
    "                    \"image_extension\": image_ext,\n",
    "                    \"image_size\": image.size  # (width, height)\n",
    "                }\n",
    "                imageMetadata.append(metadata)\n",
    "\n",
    "#             # Extract tables\n",
    "#             table = plumber_page.extract_table()\n",
    "#             if table:\n",
    "#                 tables.append(pd.DataFrame(table))\n",
    "    \n",
    "    # Extract text from images using OCR (optional)\n",
    "    ocr_texts = ocr_images(images)\n",
    "    for ocr_text in ocr_texts:\n",
    "        metadata = {\n",
    "            \"file\": file_name,\n",
    "            \"page_number\": page_num + 1,  # 1-based index\n",
    "            \"chunk_length\": len(chunk)\n",
    "                    }\n",
    "        text_chunks.append(ocr_text)\n",
    "        textMetadata.append(metadata)\n",
    "    \n",
    "    return {\n",
    "        \"text_chunks\": text_chunks,\n",
    "        \"images\": images,\n",
    "#         \"tables\": tables,\n",
    "        \"text_metadata\": textMetadata,\n",
    "        \"image_metadata\": imageMetadata\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28073d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_webpage_content(url):\n",
    "    text_chunks = []\n",
    "    text_metadata = []\n",
    "    images = []\n",
    "    image_metadata = []\n",
    "    \n",
    "    # Fetch and parse the web page\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Extract and process text content\n",
    "    text_content = ' '.join([p.get_text() for p in soup.find_all(['p', 'h1', 'h2', 'h3'])]).strip()\n",
    "    if text_content:\n",
    "        # Assume you have defined the clean_text and chunk_text functions\n",
    "        cleaned_text = clean_text(text_content)\n",
    "        chunks = chunk_text(cleaned_text)\n",
    "        text_chunks.extend(chunks)\n",
    "        \n",
    "        # Collect text metadata\n",
    "        for chunk in chunks:\n",
    "            metadata = {\n",
    "                \"file\": url.replace('https://', '').replace('http://', '').split('/')[0],  # Use domain as file name\n",
    "                \"page_number\": None,\n",
    "                \"chunk_length\": len(chunk)\n",
    "            }\n",
    "            text_metadata.append(metadata)\n",
    "\n",
    "    # Extract and process images\n",
    "    image_urls = [img['src'] for img in soup.find_all('img')]\n",
    "    for img_index, img_url in enumerate(image_urls):\n",
    "        try:\n",
    "            # Handle relative URLs\n",
    "            if not img_url.startswith('http'):\n",
    "                img_url = requests.compat.urljoin(url, img_url)\n",
    "\n",
    "            img_response = requests.get(img_url)\n",
    "            image = Image.open(io.BytesIO(img_response.content))\n",
    "            images.append(image)\n",
    "\n",
    "            # Collect image metadata\n",
    "            metadata = {\n",
    "                \"file\": url.replace('https://', '').replace('http://', '').split('/')[0],  # Use domain as file name\n",
    "                \"page_number\": None,\n",
    "                \"image_index\": img_index,\n",
    "                \"image_extension\": img_url,\n",
    "                \"image_size\": image.size  # (width, height)\n",
    "            }\n",
    "            image_metadata.append(metadata)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {img_url}: {e}\")\n",
    "\n",
    "    return {\n",
    "        \"text_chunks\": text_chunks,\n",
    "        \"text_metadata\": text_metadata,\n",
    "        \"images\": images,\n",
    "        \"image_metadata\": image_metadata\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8cf5a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdfs = [\"Data/OperationalManagementPlan.pdf\", \n",
    "        \"Data/BoxGumGrassyWoodlandNationalRecoveryPlan.pdf\",\n",
    "        \"Data/EnvironmentalOffsets.pdf\",\n",
    "        \"Data/NatureConservationAct.pdf\",\n",
    "        \"Data/ReserveManagementPlan.pdf\",\n",
    "        \"Data/StateOfEnvironment.pdf\",\n",
    "        \"Data/WoodlandConservationStrategy.pdf\",\n",
    "        \"Data/WoodlandConservationStrategyBoxGumGrassyWoodland.pdf\",\n",
    "        \"Data/WatsonWoodlandsWorkingGroup.pdf\"]\n",
    "\n",
    "webpages = [\"https://www.parks.act.gov.au/find-a-park/canberra-nature-park/justice-robert-hope-park\",\n",
    "            \"https://www.environment.act.gov.au/ACT-parks-conservation/environmental-offsets/individual-projects/justice-robert-hope-park-offset-area\",\n",
    "            \"https://greens.org.au/act/news/act-greens-act-protect-act-endangered-woodlands-development-0\"]\n",
    "\n",
    "for pdf in pdfs:\n",
    "    extracted_content = extract_pdf_content(pdf)\n",
    "    image_archive.extend(extracted_content['images'])\n",
    "    image_metadata.extend(extracted_content['image_metadata'])\n",
    "    text_archive.extend(extracted_content['text_chunks'])\n",
    "    text_metadata.extend(extracted_content['text_metadata'])\n",
    "    \n",
    "for webpage in webpages:\n",
    "    extracted_content = extract_webpage_content(webpage)\n",
    "    image_archive.extend(extracted_content['images'])\n",
    "    image_metadata.extend(extracted_content['image_metadata'])\n",
    "    text_archive.extend(extracted_content['text_chunks'])\n",
    "    text_metadata.extend(extracted_content['text_metadata'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19d6835d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NEXT STEP - PROCESS ALA DATA\n",
    "\n",
    "ala_data = pd.read_csv('Data/ALA.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6f32811",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NEXT STEP - PROCESS WEATHER DATA\n",
    "\n",
    "weather_data = pd.read_csv('Data/Weather.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a95a9554",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NEXT STEP - PROCESS SATELLITE IMAGES\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b0ae5943",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NEXT STEP - PROCESS IMAGE ARCHIVE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "d04e232c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONSIDER METADATA\n",
    "\n",
    "text_archive = [text for text in text_archive if text]\n",
    "image_archive = [image for image in image_archive if image]\n",
    "\n",
    "# Assuming embeddings are stored in a list\n",
    "# For text: text_embeddings = [get_transformer_embeddings(text) for text in texts]\n",
    "# For images: image_embeddings = [get_image_embedding(image_path) for image_path in image_paths]\n",
    "\n",
    "text_embeddings = [get_text_embedding(text).data[0].embedding for text in text_archive]\n",
    "image_embeddings = [get_image_embedding(image).tolist()[0] for image in image_archive]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "e08c070b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embeddings_database = np.array(text_embeddings)\n",
    "image_embeddings_database = np.array(image_embeddings)\n",
    "\n",
    "text_dimension = text_embeddings_database.shape[1]\n",
    "text_index = faiss.IndexFlatL2(text_dimension)\n",
    "text_index.add(text_embeddings_database)\n",
    "faiss.write_index(text_index, \"textArchive.index\")\n",
    "\n",
    "image_dimension = image_embeddings_database.shape[1]\n",
    "image_index = faiss.IndexFlatL2(image_dimension)\n",
    "image_index.add(image_embeddings_database)\n",
    "faiss.write_index(image_index, \"imageArchive.index\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
